# üßó‚Äç‚ôÇÔ∏è Indoor Climbing Hold Segmentation (Detectron2)

This project builds an end-to-end **computer vision pipeline** that segments **holds**, **volumes**, and **downclimb markers** on indoor climbing walls using **Mask R-CNN** (via Facebook AI‚Äôs [Detectron2](https://github.com/facebookresearch/detectron2)).

It combines **dataset handling**, **COCO annotation registration**, **model training/evaluation**, and **visualization** into one workflow ‚Äî enabling automatic route understanding in climbing gyms.

---

## üìÇ Project Structure

| Path | Description |
|------|--------------|
| `src/hold_detectron.py` | Reproducible CLI script ‚Äî trains, evaluates, and saves sample visualizations. |
| `notebooks/hold_detectron2_notebook.ipynb` | Readable Colab-style notebook ‚Äî step-by-step exploration of the same pipeline. |
| `outputs/metrics.json` | Training metrics summary (mAP, AP per class, etc.). |
| `outputs/examples/` | Example segmentation results (see below). |
| `models/README.md` | Link to the trained `model_final.pth` weights hosted on Google Drive. |
| `requirements.txt` | Core dependencies (PyTorch, OpenCV, etc.). |

---

## üñºÔ∏è Example Results

Below are example outputs generated by the trained model:

<p float="left">
  <img src="outputs/examples/example1.png" width="420"/>
  <img src="outputs/examples/example2.png" width="420"/>
</p>

Each color overlay represents a predicted **hold**, **volume**, or **downclimb** region.

---

## üß± Dataset Setup

The project expects **three datasets** (Task A, B, C) in **COCO format** ‚Äî each containing an `instances_default.json` file and its corresponding `images/` folder.

/path/to/taskA/
‚îú‚îÄ‚îÄ images/
‚îî‚îÄ‚îÄ instances_default.json
/path/to/taskB/
‚îú‚îÄ‚îÄ images/
‚îî‚îÄ‚îÄ instances_default.json
/path/to/taskC/
‚îú‚îÄ‚îÄ images/
‚îî‚îÄ‚îÄ instances_default.json

> üì© **Data Access:**  
> The full Task A/B/C datasets and annotations are **not included** due to size and privacy.  
> If you‚Äôd like to reproduce the experiment, please **contact me** to request access.

---

## üì∏ Data Collection & Annotation

All images were **collected from a local indoor climbing gym**, capturing walls under different lighting and angles to ensure diversity in hold colors and surface textures.  
Each image was annotated using the open-source tool **[CVAT (Computer Vision Annotation Tool)](https://cvat.org/)**.

- **Annotation type:** polygonal segmentation  
- **Categories:** `hold`, `volume`, `downclimb`  
- **Format:** exported to **COCO JSON** (`instances_default.json`) for Detectron2 compatibility  
- **Quality control:** each annotated frame was reviewed manually to ensure clean contours and correct class labeling  
- **Dataset split:** merged across three subsets (Task A, B, C) and divided into training, validation, and test splits with a ratio of 70 : 10 : 20

---

## ‚öôÔ∏è Installation

> üí° **Windows users:** Detectron2 doesn‚Äôt provide official Windows wheels.  
> Use **WSL2 + Ubuntu** or a Linux environment for best results.

1. **Create and activate a virtual environment**
```bash
# Linux / macOS
python -m venv .venv && source .venv/bin/activate
# Windows PowerShell
python -m venv .venv; .\.venv\Scripts\Activate.ps1
```

### 2. Install core dependencies
```bash
pip install -r requirements.txt
```

### 3. Install PyTorch + Detectron2

Visit pytorch.org to choose the correct command for your OS/CUDA.

Then install Detectron2 using the matching wheel:

```bash
# Example (CUDA 12.1 + Torch 2.2)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu121/torch2.2/index.html
```

---

## üöÄ Usage

### ‚úÖ Train + Evaluate + Visualize

```bash
python src/hold_detectron.py \
--taskA_json /path/to/taskA/instances_default.json \
--taskA_root /path/to/taskA/images \
--taskB_json /path/to/taskB/instances_default.json \
--taskB_root /path/to/taskB/images \
--taskC_json /path/to/taskC/instances_default.json \
--taskC_root /path/to/taskC/images \
--output_dir outputs \
--max_iter 8000
```

Training progress and metrics appear in the terminal.

Evaluation results are saved under outputs/eval_boulder_val/ and outputs/eval_boulder_test/.

Visualized predictions are saved to outputs/viz_samples/.

---

### üß© Evaluate only (skip training)
If you already have a trained model:

```bash
python src/hold_detectron.py \
--taskA_json /path/to/taskA/instances_default.json \
--taskA_root /path/to/taskA/images \
--taskB_json /path/to/taskB/instances_default.json \
--taskB_root /path/to/taskB/images \
--taskC_json /path/to/taskC/instances_default.json \
--taskC_root /path/to/taskC/images \
--output_dir outputs \
--skip_train \
--weights models/model_final.pth
```

---

## üß† Notebook vs Script

| Format | Purpose |
|---------|----------|
| **`.ipynb`** | Interactive learning and visualization (ideal for Google Colab or Jupyter). |
| **`.py`** | Command-line reproducibility ‚Äî clean, parameterized training & evaluation. |

Use the notebook to **understand the process**, and the script for **consistent experiments**.

---

## üîÆ Future Work

- Expand dataset variety (lighting, wall texture, manufacturer differences).  
- Introduce **hold-type classification** ‚Äî train a secondary model to recognize specific hold shapes such as *jugs, crimps, pockets, slopers,* and *pinches.*  
- Experiment with **synthetic data generation** or **self-supervised pretraining** to improve model robustness with limited data.  
- Explore **route difficulty estimation** by combining detected hold types, spacing, and wall geometry.  
- Develop **lightweight on-device inference** or a **simple web/mobile demo** for climbers and gym route setters.

---

üìù License & Attribution

Code released for research and educational purposes.

Built using Detectron2 by Meta AI.

Please ensure you have rights to any climbing imagery used for reproduction.

Author: Yunki Cho

üìß Contact for dataset or collaboration inquiries.

yunki6227@gmail.com

---